# Copyright (c) 2024 Helixon Limited.
#
# This file is a part of ProtMyth and is released under the MIT License.
# Thanks for using ProtMyth!

"""
Axial Attention Modules from ESM
"""

from torch import nn


class RowSelfAttention(nn.Module):
    """
    RowSelfAttention
    """

    def __init__(self) -> None:
        super().__init__()


class ColumnSelfAttention(nn.Module):
    """
    ColumnSelfAttention
    """

    def __init__(self) -> None:
        super().__init__()
